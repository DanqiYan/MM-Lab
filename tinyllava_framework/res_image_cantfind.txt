### Starting TaskPrologue of job 2071923 on a0536 at Sat Sep 28 02:28:02 CEST 2024
Running on cores 0-127 with governor ondemand
Sat Sep 28 02:28:02 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0E:00.0 Off |                    0 |
| N/A   34C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:13:00.0 Off |                    0 |
| N/A   36C    P0             67W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:49:00.0 Off |                    0 |
| N/A   34C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:4F:00.0 Off |                    0 |
| N/A   36C    P0             67W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100-SXM4-80GB          On  |   00000000:90:00.0 Off |                    0 |
| N/A   36C    P0             63W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100-SXM4-80GB          On  |   00000000:96:00.0 Off |                    0 |
| N/A   33C    P0             63W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100-SXM4-80GB          On  |   00000000:CC:00.0 Off |                    0 |
| N/A   34C    P0             65W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100-SXM4-80GB          On  |   00000000:D1:00.0 Off |                    0 |
| N/A   35C    P0             61W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

a0536.nhr.fau.de
Sat Sep 28 02:28:05 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0E:00.0 Off |                    0 |
| N/A   34C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:13:00.0 Off |                    0 |
| N/A   36C    P0             67W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:49:00.0 Off |                    0 |
| N/A   34C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:4F:00.0 Off |                    0 |
| N/A   36C    P0             72W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100-SXM4-80GB          On  |   00000000:90:00.0 Off |                    0 |
| N/A   36C    P0             68W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A100-SXM4-80GB          On  |   00000000:96:00.0 Off |                    0 |
| N/A   34C    P0             79W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A100-SXM4-80GB          On  |   00000000:CC:00.0 Off |                    0 |
| N/A   34C    P0             75W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A100-SXM4-80GB          On  |   00000000:D1:00.0 Off |                    0 |
| N/A   35C    P0             73W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[2024-09-28 02:28:35,456] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-09-28 02:29:04,592] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-09-28 02:29:04,592] [INFO] [runner.py:585:main] cmd = /home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29501 --enable_each_rank_log=None /home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py --deepspeed /home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json --data_path /home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json --image_folder /home/atuin/b211dd/b211dd20/dataset --is_multimodal True --conv_version phi --model_name_or_path microsoft/phi-2 --vision_tower google/siglip-so400m-patch14-384 --vision_tower2  --connector_type mlp2x_gelu --mm_vision_select_layer -2 --image_aspect_ratio square --attn_implementation flash_attention_2 --fp16 True --training_recipe common --tune_type_llm full --tune_type_vision_tower frozen --tune_vision_tower_from_layer 0 --tune_type_connector full --group_by_modality_length True --pretrained_model_path /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain --output_dir /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 False --model_max_length 3072 --gradient_checkpointing True --dataloader_num_workers 8 --lazy_preprocess True --report_to wandb --tokenizer_use_fast False --run_name tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full
[2024-09-28 02:29:06,604] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-09-28 02:29:09,352] [INFO] [launch.py:139:main] 0 TORCH_NCCL_ASYNC_ERROR_HANDLING=1
[2024-09-28 02:29:09,352] [INFO] [launch.py:139:main] 0 TORCH_NCCL_BLOCKING_WAIT=1
[2024-09-28 02:29:09,352] [INFO] [launch.py:139:main] 0 NCCL_TIMEOUT=1800
[2024-09-28 02:29:09,353] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-09-28 02:29:09,353] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-09-28 02:29:09,353] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-09-28 02:29:09,353] [INFO] [launch.py:164:main] dist_world_size=8
[2024-09-28 02:29:09,353] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-09-28 02:29:09,354] [INFO] [launch.py:256:main] process 2473970 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=0', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,355] [INFO] [launch.py:256:main] process 2473971 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=1', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,356] [INFO] [launch.py:256:main] process 2473972 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=2', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,356] [INFO] [launch.py:256:main] process 2473973 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=3', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,357] [INFO] [launch.py:256:main] process 2473974 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=4', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,358] [INFO] [launch.py:256:main] process 2473975 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=5', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,359] [INFO] [launch.py:256:main] process 2473976 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=6', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:09,360] [INFO] [launch.py:256:main] process 2473977 spawned with command: ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=7', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full']
[2024-09-28 02:29:39,718] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-28 02:29:39,753] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-28 02:29:39,772] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-28 02:29:40,171] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-09-28 02:29:40,184] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-09-28 02:29:45,236] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-28 02:29:45,280] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2024-09-28 02:29:51,279] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hpc/b211dd/b211dd20/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.

nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.
nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.
nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.
nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.


[2024-09-28 02:30:09,817] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,817] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,817] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-09-28 02:30:09,817] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,818] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,818] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,818] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,818] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-28 02:30:09,819] [INFO] [comm.py:652:init_distributed] cdb=None
Before loading, model is on device: cpu
Before loading, model is on device: cpu
Before loading, model is on device: cpu
[2024-09-28 02:32:35,568] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
Before loading, model is on device: cpu
Before loading, model is on device: cpu
Before loading, model is on device: cpu
Before loading, model is on device: cpu
Before loading, model is on device: cpu
[2024-09-28 02:33:22,552] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:22,552] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:22,552] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:22,552] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:22,553] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:22,554] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:22,562] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:33:29,309] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 453, num_elems = 2.78B
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
loading language model from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/language_model
[2024-09-28 02:34:03,745] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,673] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,673] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,673] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,673] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,673] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,674] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:06,674] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2024-09-28 02:34:08,713] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 901, num_elems = 3.21B
Loading vision tower from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
Loading vision tower from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
Loading vision tower from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
Loading vision tower from Loading vision tower from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
 /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_towerLoading vision tower from 
 /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
Loading vision tower from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
Loading vision tower from  /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/vision_tower
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...After loading, model is on device: cuda:7
After loading, model is on device: cuda:1

After loading, model is on device: cuda:3
After loading, model is on device: cuda:6
After loading, model is on device: cuda:2
Loading connector from /home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain/connector/pytorch_model.bin...
After loading, model is on device: cuda:5
After loading, model is on device: cuda:4
After loading, model is on device: cuda:0
2024-09-28 02:34:36,311 | INFO: Total Parameters: 9507840, Total Trainable Parameters: 9507840
2024-09-28 02:34:36,311 | INFO: Trainable Parameters:
language_model.model.embed_tokens.weight: 0 parameters
language_model.model.layers.0.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.0.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.0.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.0.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.0.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.0.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.0.self_attn.dense.weight: 0 parameters
language_model.model.layers.0.self_attn.dense.bias: 0 parameters
language_model.model.layers.0.mlp.fc1.weight: 0 parameters
language_model.model.layers.0.mlp.fc1.bias: 0 parameters
language_model.model.layers.0.mlp.fc2.weight: 0 parameters
language_model.model.layers.0.mlp.fc2.bias: 0 parameters
language_model.model.layers.0.input_layernorm.weight: 0 parameters
language_model.model.layers.0.input_layernorm.bias: 0 parameters
language_model.model.layers.1.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.1.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.1.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.1.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.1.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.1.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.1.self_attn.dense.weight: 0 parameters
language_model.model.layers.1.self_attn.dense.bias: 0 parameters
language_model.model.layers.1.mlp.fc1.weight: 0 parameters
language_model.model.layers.1.mlp.fc1.bias: 0 parameters
language_model.model.layers.1.mlp.fc2.weight: 0 parameters
language_model.model.layers.1.mlp.fc2.bias: 0 parameters
language_model.model.layers.1.input_layernorm.weight: 0 parameters
language_model.model.layers.1.input_layernorm.bias: 0 parameters
language_model.model.layers.2.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.2.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.2.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.2.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.2.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.2.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.2.self_attn.dense.weight: 0 parameters
language_model.model.layers.2.self_attn.dense.bias: 0 parameters
language_model.model.layers.2.mlp.fc1.weight: 0 parameters
language_model.model.layers.2.mlp.fc1.bias: 0 parameters
language_model.model.layers.2.mlp.fc2.weight: 0 parameters
language_model.model.layers.2.mlp.fc2.bias: 0 parameters
language_model.model.layers.2.input_layernorm.weight: 0 parameters
language_model.model.layers.2.input_layernorm.bias: 0 parameters
language_model.model.layers.3.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.3.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.3.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.3.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.3.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.3.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.3.self_attn.dense.weight: 0 parameters
language_model.model.layers.3.self_attn.dense.bias: 0 parameters
language_model.model.layers.3.mlp.fc1.weight: 0 parameters
language_model.model.layers.3.mlp.fc1.bias: 0 parameters
language_model.model.layers.3.mlp.fc2.weight: 0 parameters
language_model.model.layers.3.mlp.fc2.bias: 0 parameters
language_model.model.layers.3.input_layernorm.weight: 0 parameters
language_model.model.layers.3.input_layernorm.bias: 0 parameters
language_model.model.layers.4.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.4.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.4.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.4.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.4.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.4.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.4.self_attn.dense.weight: 0 parameters
language_model.model.layers.4.self_attn.dense.bias: 0 parameters
language_model.model.layers.4.mlp.fc1.weight: 0 parameters
language_model.model.layers.4.mlp.fc1.bias: 0 parameters
language_model.model.layers.4.mlp.fc2.weight: 0 parameters
language_model.model.layers.4.mlp.fc2.bias: 0 parameters
language_model.model.layers.4.input_layernorm.weight: 0 parameters
language_model.model.layers.4.input_layernorm.bias: 0 parameters
language_model.model.layers.5.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.5.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.5.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.5.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.5.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.5.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.5.self_attn.dense.weight: 0 parameters
language_model.model.layers.5.self_attn.dense.bias: 0 parameters
language_model.model.layers.5.mlp.fc1.weight: 0 parameters
language_model.model.layers.5.mlp.fc1.bias: 0 parameters
language_model.model.layers.5.mlp.fc2.weight: 0 parameters
language_model.model.layers.5.mlp.fc2.bias: 0 parameters
language_model.model.layers.5.input_layernorm.weight: 0 parameters
language_model.model.layers.5.input_layernorm.bias: 0 parameters
language_model.model.layers.6.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.6.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.6.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.6.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.6.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.6.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.6.self_attn.dense.weight: 0 parameters
language_model.model.layers.6.self_attn.dense.bias: 0 parameters
language_model.model.layers.6.mlp.fc1.weight: 0 parameters
language_model.model.layers.6.mlp.fc1.bias: 0 parameters
language_model.model.layers.6.mlp.fc2.weight: 0 parameters
language_model.model.layers.6.mlp.fc2.bias: 0 parameters
language_model.model.layers.6.input_layernorm.weight: 0 parameters
language_model.model.layers.6.input_layernorm.bias: 0 parameters
language_model.model.layers.7.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.7.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.7.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.7.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.7.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.7.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.7.self_attn.dense.weight: 0 parameters
language_model.model.layers.7.self_attn.dense.bias: 0 parameters
language_model.model.layers.7.mlp.fc1.weight: 0 parameters
language_model.model.layers.7.mlp.fc1.bias: 0 parameters
language_model.model.layers.7.mlp.fc2.weight: 0 parameters
language_model.model.layers.7.mlp.fc2.bias: 0 parameters
language_model.model.layers.7.input_layernorm.weight: 0 parameters
language_model.model.layers.7.input_layernorm.bias: 0 parameters
language_model.model.layers.8.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.8.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.8.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.8.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.8.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.8.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.8.self_attn.dense.weight: 0 parameters
language_model.model.layers.8.self_attn.dense.bias: 0 parameters
language_model.model.layers.8.mlp.fc1.weight: 0 parameters
language_model.model.layers.8.mlp.fc1.bias: 0 parameters
language_model.model.layers.8.mlp.fc2.weight: 0 parameters
language_model.model.layers.8.mlp.fc2.bias: 0 parameters
language_model.model.layers.8.input_layernorm.weight: 0 parameters
language_model.model.layers.8.input_layernorm.bias: 0 parameters
language_model.model.layers.9.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.9.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.9.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.9.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.9.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.9.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.9.self_attn.dense.weight: 0 parameters
language_model.model.layers.9.self_attn.dense.bias: 0 parameters
language_model.model.layers.9.mlp.fc1.weight: 0 parameters
language_model.model.layers.9.mlp.fc1.bias: 0 parameters
language_model.model.layers.9.mlp.fc2.weight: 0 parameters
language_model.model.layers.9.mlp.fc2.bias: 0 parameters
language_model.model.layers.9.input_layernorm.weight: 0 parameters
language_model.model.layers.9.input_layernorm.bias: 0 parameters
language_model.model.layers.10.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.10.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.10.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.10.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.10.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.10.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.10.self_attn.dense.weight: 0 parameters
language_model.model.layers.10.self_attn.dense.bias: 0 parameters
language_model.model.layers.10.mlp.fc1.weight: 0 parameters
language_model.model.layers.10.mlp.fc1.bias: 0 parameters
language_model.model.layers.10.mlp.fc2.weight: 0 parameters
language_model.model.layers.10.mlp.fc2.bias: 0 parameters
language_model.model.layers.10.input_layernorm.weight: 0 parameters
language_model.model.layers.10.input_layernorm.bias: 0 parameters
language_model.model.layers.11.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.11.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.11.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.11.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.11.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.11.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.11.self_attn.dense.weight: 0 parameters
language_model.model.layers.11.self_attn.dense.bias: 0 parameters
language_model.model.layers.11.mlp.fc1.weight: 0 parameters
language_model.model.layers.11.mlp.fc1.bias: 0 parameters
language_model.model.layers.11.mlp.fc2.weight: 0 parameters
language_model.model.layers.11.mlp.fc2.bias: 0 parameters
language_model.model.layers.11.input_layernorm.weight: 0 parameters
language_model.model.layers.11.input_layernorm.bias: 0 parameters
language_model.model.layers.12.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.12.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.12.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.12.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.12.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.12.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.12.self_attn.dense.weight: 0 parameters
language_model.model.layers.12.self_attn.dense.bias: 0 parameters
language_model.model.layers.12.mlp.fc1.weight: 0 parameters
language_model.model.layers.12.mlp.fc1.bias: 0 parameters
language_model.model.layers.12.mlp.fc2.weight: 0 parameters
language_model.model.layers.12.mlp.fc2.bias: 0 parameters
language_model.model.layers.12.input_layernorm.weight: 0 parameters
language_model.model.layers.12.input_layernorm.bias: 0 parameters
language_model.model.layers.13.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.13.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.13.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.13.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.13.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.13.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.13.self_attn.dense.weight: 0 parameters
language_model.model.layers.13.self_attn.dense.bias: 0 parameters
language_model.model.layers.13.mlp.fc1.weight: 0 parameters
language_model.model.layers.13.mlp.fc1.bias: 0 parameters
language_model.model.layers.13.mlp.fc2.weight: 0 parameters
language_model.model.layers.13.mlp.fc2.bias: 0 parameters
language_model.model.layers.13.input_layernorm.weight: 0 parameters
language_model.model.layers.13.input_layernorm.bias: 0 parameters
language_model.model.layers.14.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.14.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.14.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.14.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.14.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.14.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.14.self_attn.dense.weight: 0 parameters
language_model.model.layers.14.self_attn.dense.bias: 0 parameters
language_model.model.layers.14.mlp.fc1.weight: 0 parameters
language_model.model.layers.14.mlp.fc1.bias: 0 parameters
language_model.model.layers.14.mlp.fc2.weight: 0 parameters
language_model.model.layers.14.mlp.fc2.bias: 0 parameters
language_model.model.layers.14.input_layernorm.weight: 0 parameters
language_model.model.layers.14.input_layernorm.bias: 0 parameters
language_model.model.layers.15.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.15.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.15.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.15.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.15.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.15.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.15.self_attn.dense.weight: 0 parameters
language_model.model.layers.15.self_attn.dense.bias: 0 parameters
language_model.model.layers.15.mlp.fc1.weight: 0 parameters
language_model.model.layers.15.mlp.fc1.bias: 0 parameters
language_model.model.layers.15.mlp.fc2.weight: 0 parameters
language_model.model.layers.15.mlp.fc2.bias: 0 parameters
language_model.model.layers.15.input_layernorm.weight: 0 parameters
language_model.model.layers.15.input_layernorm.bias: 0 parameters
language_model.model.layers.16.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.16.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.16.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.16.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.16.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.16.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.16.self_attn.dense.weight: 0 parameters
language_model.model.layers.16.self_attn.dense.bias: 0 parameters
language_model.model.layers.16.mlp.fc1.weight: 0 parameters
language_model.model.layers.16.mlp.fc1.bias: 0 parameters
language_model.model.layers.16.mlp.fc2.weight: 0 parameters
language_model.model.layers.16.mlp.fc2.bias: 0 parameters
language_model.model.layers.16.input_layernorm.weight: 0 parameters
language_model.model.layers.16.input_layernorm.bias: 0 parameters
language_model.model.layers.17.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.17.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.17.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.17.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.17.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.17.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.17.self_attn.dense.weight: 0 parameters
language_model.model.layers.17.self_attn.dense.bias: 0 parameters
language_model.model.layers.17.mlp.fc1.weight: 0 parameters
language_model.model.layers.17.mlp.fc1.bias: 0 parameters
language_model.model.layers.17.mlp.fc2.weight: 0 parameters
language_model.model.layers.17.mlp.fc2.bias: 0 parameters
language_model.model.layers.17.input_layernorm.weight: 0 parameters
language_model.model.layers.17.input_layernorm.bias: 0 parameters
language_model.model.layers.18.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.18.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.18.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.18.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.18.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.18.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.18.self_attn.dense.weight: 0 parameters
language_model.model.layers.18.self_attn.dense.bias: 0 parameters
language_model.model.layers.18.mlp.fc1.weight: 0 parameters
language_model.model.layers.18.mlp.fc1.bias: 0 parameters
language_model.model.layers.18.mlp.fc2.weight: 0 parameters
language_model.model.layers.18.mlp.fc2.bias: 0 parameters
language_model.model.layers.18.input_layernorm.weight: 0 parameters
language_model.model.layers.18.input_layernorm.bias: 0 parameters
language_model.model.layers.19.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.19.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.19.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.19.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.19.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.19.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.19.self_attn.dense.weight: 0 parameters
language_model.model.layers.19.self_attn.dense.bias: 0 parameters
language_model.model.layers.19.mlp.fc1.weight: 0 parameters
language_model.model.layers.19.mlp.fc1.bias: 0 parameters
language_model.model.layers.19.mlp.fc2.weight: 0 parameters
language_model.model.layers.19.mlp.fc2.bias: 0 parameters
language_model.model.layers.19.input_layernorm.weight: 0 parameters
language_model.model.layers.19.input_layernorm.bias: 0 parameters
language_model.model.layers.20.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.20.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.20.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.20.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.20.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.20.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.20.self_attn.dense.weight: 0 parameters
language_model.model.layers.20.self_attn.dense.bias: 0 parameters
language_model.model.layers.20.mlp.fc1.weight: 0 parameters
language_model.model.layers.20.mlp.fc1.bias: 0 parameters
language_model.model.layers.20.mlp.fc2.weight: 0 parameters
language_model.model.layers.20.mlp.fc2.bias: 0 parameters
language_model.model.layers.20.input_layernorm.weight: 0 parameters
language_model.model.layers.20.input_layernorm.bias: 0 parameters
language_model.model.layers.21.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.21.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.21.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.21.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.21.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.21.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.21.self_attn.dense.weight: 0 parameters
language_model.model.layers.21.self_attn.dense.bias: 0 parameters
language_model.model.layers.21.mlp.fc1.weight: 0 parameters
language_model.model.layers.21.mlp.fc1.bias: 0 parameters
language_model.model.layers.21.mlp.fc2.weight: 0 parameters
language_model.model.layers.21.mlp.fc2.bias: 0 parameters
language_model.model.layers.21.input_layernorm.weight: 0 parameters
language_model.model.layers.21.input_layernorm.bias: 0 parameters
language_model.model.layers.22.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.22.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.22.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.22.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.22.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.22.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.22.self_attn.dense.weight: 0 parameters
language_model.model.layers.22.self_attn.dense.bias: 0 parameters
language_model.model.layers.22.mlp.fc1.weight: 0 parameters
language_model.model.layers.22.mlp.fc1.bias: 0 parameters
language_model.model.layers.22.mlp.fc2.weight: 0 parameters
language_model.model.layers.22.mlp.fc2.bias: 0 parameters
language_model.model.layers.22.input_layernorm.weight: 0 parameters
language_model.model.layers.22.input_layernorm.bias: 0 parameters
language_model.model.layers.23.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.23.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.23.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.23.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.23.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.23.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.23.self_attn.dense.weight: 0 parameters
language_model.model.layers.23.self_attn.dense.bias: 0 parameters
language_model.model.layers.23.mlp.fc1.weight: 0 parameters
language_model.model.layers.23.mlp.fc1.bias: 0 parameters
language_model.model.layers.23.mlp.fc2.weight: 0 parameters
language_model.model.layers.23.mlp.fc2.bias: 0 parameters
language_model.model.layers.23.input_layernorm.weight: 0 parameters
language_model.model.layers.23.input_layernorm.bias: 0 parameters
language_model.model.layers.24.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.24.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.24.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.24.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.24.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.24.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.24.self_attn.dense.weight: 0 parameters
language_model.model.layers.24.self_attn.dense.bias: 0 parameters
language_model.model.layers.24.mlp.fc1.weight: 0 parameters
language_model.model.layers.24.mlp.fc1.bias: 0 parameters
language_model.model.layers.24.mlp.fc2.weight: 0 parameters
language_model.model.layers.24.mlp.fc2.bias: 0 parameters
language_model.model.layers.24.input_layernorm.weight: 0 parameters
language_model.model.layers.24.input_layernorm.bias: 0 parameters
language_model.model.layers.25.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.25.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.25.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.25.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.25.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.25.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.25.self_attn.dense.weight: 0 parameters
language_model.model.layers.25.self_attn.dense.bias: 0 parameters
language_model.model.layers.25.mlp.fc1.weight: 0 parameters
language_model.model.layers.25.mlp.fc1.bias: 0 parameters
language_model.model.layers.25.mlp.fc2.weight: 0 parameters
language_model.model.layers.25.mlp.fc2.bias: 0 parameters
language_model.model.layers.25.input_layernorm.weight: 0 parameters
language_model.model.layers.25.input_layernorm.bias: 0 parameters
language_model.model.layers.26.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.26.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.26.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.26.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.26.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.26.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.26.self_attn.dense.weight: 0 parameters
language_model.model.layers.26.self_attn.dense.bias: 0 parameters
language_model.model.layers.26.mlp.fc1.weight: 0 parameters
language_model.model.layers.26.mlp.fc1.bias: 0 parameters
language_model.model.layers.26.mlp.fc2.weight: 0 parameters
language_model.model.layers.26.mlp.fc2.bias: 0 parameters
language_model.model.layers.26.input_layernorm.weight: 0 parameters
language_model.model.layers.26.input_layernorm.bias: 0 parameters
language_model.model.layers.27.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.27.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.27.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.27.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.27.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.27.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.27.self_attn.dense.weight: 0 parameters
language_model.model.layers.27.self_attn.dense.bias: 0 parameters
language_model.model.layers.27.mlp.fc1.weight: 0 parameters
language_model.model.layers.27.mlp.fc1.bias: 0 parameters
language_model.model.layers.27.mlp.fc2.weight: 0 parameters
language_model.model.layers.27.mlp.fc2.bias: 0 parameters
language_model.model.layers.27.input_layernorm.weight: 0 parameters
language_model.model.layers.27.input_layernorm.bias: 0 parameters
language_model.model.layers.28.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.28.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.28.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.28.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.28.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.28.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.28.self_attn.dense.weight: 0 parameters
language_model.model.layers.28.self_attn.dense.bias: 0 parameters
language_model.model.layers.28.mlp.fc1.weight: 0 parameters
language_model.model.layers.28.mlp.fc1.bias: 0 parameters
language_model.model.layers.28.mlp.fc2.weight: 0 parameters
language_model.model.layers.28.mlp.fc2.bias: 0 parameters
language_model.model.layers.28.input_layernorm.weight: 0 parameters
language_model.model.layers.28.input_layernorm.bias: 0 parameters
language_model.model.layers.29.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.29.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.29.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.29.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.29.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.29.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.29.self_attn.dense.weight: 0 parameters
language_model.model.layers.29.self_attn.dense.bias: 0 parameters
language_model.model.layers.29.mlp.fc1.weight: 0 parameters
language_model.model.layers.29.mlp.fc1.bias: 0 parameters
language_model.model.layers.29.mlp.fc2.weight: 0 parameters
language_model.model.layers.29.mlp.fc2.bias: 0 parameters
language_model.model.layers.29.input_layernorm.weight: 0 parameters
language_model.model.layers.29.input_layernorm.bias: 0 parameters
language_model.model.layers.30.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.30.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.30.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.30.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.30.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.30.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.30.self_attn.dense.weight: 0 parameters
language_model.model.layers.30.self_attn.dense.bias: 0 parameters
language_model.model.layers.30.mlp.fc1.weight: 0 parameters
language_model.model.layers.30.mlp.fc1.bias: 0 parameters
language_model.model.layers.30.mlp.fc2.weight: 0 parameters
language_model.model.layers.30.mlp.fc2.bias: 0 parameters
language_model.model.layers.30.input_layernorm.weight: 0 parameters
language_model.model.layers.30.input_layernorm.bias: 0 parameters
language_model.model.layers.31.self_attn.q_proj.weight: 0 parameters
language_model.model.layers.31.self_attn.q_proj.bias: 0 parameters
language_model.model.layers.31.self_attn.k_proj.weight: 0 parameters
language_model.model.layers.31.self_attn.k_proj.bias: 0 parameters
language_model.model.layers.31.self_attn.v_proj.weight: 0 parameters
language_model.model.layers.31.self_attn.v_proj.bias: 0 parameters
language_model.model.layers.31.self_attn.dense.weight: 0 parameters
language_model.model.layers.31.self_attn.dense.bias: 0 parameters
language_model.model.layers.31.mlp.fc1.weight: 0 parameters
language_model.model.layers.31.mlp.fc1.bias: 0 parameters
language_model.model.layers.31.mlp.fc2.weight: 0 parameters
language_model.model.layers.31.mlp.fc2.bias: 0 parameters
language_model.model.layers.31.input_layernorm.weight: 0 parameters
language_model.model.layers.31.input_layernorm.bias: 0 parameters
language_model.model.final_layernorm.weight: 0 parameters
language_model.model.final_layernorm.bias: 0 parameters
language_model.lm_head.weight: 0 parameters
language_model.lm_head.bias: 0 parameters
connector._connector.0.weight: 2949120 parameters
connector._connector.0.bias: 2560 parameters
connector._connector.2.weight: 6553600 parameters
connector._connector.2.bias: 2560 parameters
2024-09-28 02:34:36,354 | WARNING: Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 1324480 in 540 params
{'loss': 2.1311, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 2.571, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 2.5851, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 2.7469, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 2.1317, 'grad_norm': 11.502486541693175, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 2.5399, 'grad_norm': 11.502486541693175, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 2.4438, 'grad_norm': 15.209768563856827, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
[1;34mwandb[0m:
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/hpc/b211dd/b211dd20/compression/tinyllava_framework/wandb/offline-run-20240928_023001-smbfxlem[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20240928_023001-smbfxlem/logs[0m
[2024-09-28 02:38:39,995] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473970
[2024-09-28 02:38:41,788] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473971
[2024-09-28 02:39:12,121] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473972
[2024-09-28 02:39:42,189] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473973
[2024-09-28 02:39:42,190] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473974
[2024-09-28 02:40:12,252] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473975
[2024-09-28 02:40:42,304] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473976
[2024-09-28 02:41:12,356] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2473977
[2024-09-28 02:41:42,420] [ERROR] [launch.py:325:sigkill_handler] ['/home/atuin/b211dd/b211dd20/software/private/conda/envs/compression/bin/python3.10', '-u', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/tinyllava/train/train.py', '--local_rank=7', '--deepspeed', '/home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/zero3.json', '--data_path', '/home/atuin/b211dd/b211dd20/dataset/text_files/llava_v1_5_mix665k.json', '--image_folder', '/home/atuin/b211dd/b211dd20/dataset', '--is_multimodal', 'True', '--conv_version', 'phi', '--model_name_or_path', 'microsoft/phi-2', '--vision_tower', 'google/siglip-so400m-patch14-384', '--vision_tower2', '', '--connector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--image_aspect_ratio', 'square', '--attn_implementation', 'flash_attention_2', '--fp16', 'True', '--training_recipe', 'common', '--tune_type_llm', 'full', '--tune_type_vision_tower', 'frozen', '--tune_vision_tower_from_layer', '0', '--tune_type_connector', 'full', '--group_by_modality_length', 'True', '--pretrained_model_path', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-pretrain', '--output_dir', '/home/atuin/b211dd/b211dd20/tinyllava/checkpoints/llava_factory/tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'False', '--model_max_length', '3072', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '8', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--tokenizer_use_fast', 'False', '--run_name', 'tiny-llava-phi-2-siglip-so400m-patch14-384-base-finetune_full'] exits with return code = 1
=== JOB_STATISTICS ===
=== current date     : Sat Sep 28 02:41:44 CEST 2024
= Job-ID             : 2071923 on alex
= Job-Name           : finetune_phi_new
= Job-Command        : /home/hpc/b211dd/b211dd20/compression/tinyllava_framework/scripts/train/train_phi_new.slurm
= Initial workdir    : /home/hpc/b211dd/b211dd20/compression/tinyllava_framework
= Queue/Partition    : a100
= Slurm account      : b211dd with QOS=normal
= Features           : a100_80
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:13:43
= Total RAM usage    : 105.2 GiB of assigned  GiB (%)
= Node list          : a0536
= Subm/Elig/Start/End: 2024-09-28T02:27:45 / 2024-09-28T02:27:45 / 2024-09-28T02:28:00 / 2024-09-28T02:41:43
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           33.7G   104.9G   209.7G        N/A      60K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-80GB, 00000000:0E:00.0, 2473970, 24 %, 5 %, 45928 MiB, 541775 ms
NVIDIA A100-SXM4-80GB, 00000000:13:00.0, 2473971, 25 %, 4 %, 50588 MiB, 543185 ms
NVIDIA A100-SXM4-80GB, 00000000:49:00.0, 2473972, 30 %, 4 %, 39842 MiB, 561780 ms
NVIDIA A100-SXM4-80GB, 00000000:4F:00.0, 2473973, 23 %, 4 %, 45940 MiB, 534144 ms
NVIDIA A100-SXM4-80GB, 00000000:90:00.0, 2473974, 32 %, 4 %, 36364 MiB, 603172 ms
NVIDIA A100-SXM4-80GB, 00000000:96:00.0, 2473975, 35 %, 4 %, 31998 MiB, 633852 ms
NVIDIA A100-SXM4-80GB, 00000000:CC:00.0, 2473976, 38 %, 3 %, 36412 MiB, 664143 ms
NVIDIA A100-SXM4-80GB, 00000000:D1:00.0, 2473977, 41 %, 3 %, 38888 MiB, 688061 ms
